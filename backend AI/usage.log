2025-11-17 03:27:54,269 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:27:54,557 | Use pytorch device_name: cpu
2025-11-17 03:27:54,557 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:27:58,816 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 03:27:58,816 | [33mPress CTRL+C to quit[0m
2025-11-17 03:27:58,818 |  * Restarting with stat
2025-11-17 03:28:13,832 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:28:13,931 | Use pytorch device_name: cpu
2025-11-17 03:28:13,931 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:28:17,586 |  * Debugger is active!
2025-11-17 03:28:17,589 |  * Debugger PIN: 136-249-302
2025-11-17 03:31:05,094 | 192.168.2.125 - - [17/Nov/2025 03:31:05] "[33mPOST /chat HTTP/1.1[0m" 404 -
2025-11-17 03:38:19,982 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 03:38:20,003 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 127, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 03:38:20,071 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=1.14s
2025-11-17 03:38:20,072 | 192.168.2.125 - - [17/Nov/2025 03:38:20] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 03:43:26,672 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:43:31,055 |  * Restarting with stat
2025-11-17 03:44:17,410 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:44:17,555 | Use pytorch device_name: cpu
2025-11-17 03:44:17,555 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:44:22,324 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 03:44:22,324 | [33mPress CTRL+C to quit[0m
2025-11-17 03:44:22,327 |  * Restarting with stat
2025-11-17 03:44:40,583 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:44:40,704 | Use pytorch device_name: cpu
2025-11-17 03:44:40,704 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:44:45,028 |  * Debugger is active!
2025-11-17 03:44:45,044 |  * Debugger PIN: 136-249-302
2025-11-17 03:45:39,563 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 134, in chat
    response = ollama_client.chat(model=MODEL_NAME, messages=messages) # S\u1eeda \u1edf \u0111ây
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 135, in _request_raw
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download

2025-11-17 03:45:39,627 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=0.23s
2025-11-17 03:45:39,629 | 192.168.2.125 - - [17/Nov/2025 03:45:39] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 03:45:58,821 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:46:01,109 |  * Restarting with stat
2025-11-17 03:46:19,376 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:46:19,548 | Use pytorch device_name: cpu
2025-11-17 03:46:19,548 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:46:23,586 |  * Debugger is active!
2025-11-17 03:46:23,592 |  * Debugger PIN: 136-249-302
2025-11-17 03:47:38,214 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:47:40,404 |  * Restarting with stat
2025-11-17 03:47:56,228 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:47:56,331 | Use pytorch device_name: cpu
2025-11-17 03:47:56,331 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:48:00,514 |  * Debugger is active!
2025-11-17 03:48:00,518 |  * Debugger PIN: 136-249-302
2025-11-17 03:48:27,766 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:48:29,708 |  * Restarting with stat
2025-11-17 03:49:05,670 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:49:05,836 | Use pytorch device_name: cpu
2025-11-17 03:49:05,836 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:49:09,995 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 03:49:09,995 | [33mPress CTRL+C to quit[0m
2025-11-17 03:49:09,998 |  * Restarting with stat
2025-11-17 03:49:26,124 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:49:26,233 | Use pytorch device_name: cpu
2025-11-17 03:49:26,233 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:49:30,501 |  * Debugger is active!
2025-11-17 03:49:30,507 |  * Debugger PIN: 136-249-302
2025-11-17 03:49:50,675 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:49:52,342 |  * Restarting with stat
2025-11-17 03:50:32,268 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:50:32,383 | Use pytorch device_name: cpu
2025-11-17 03:50:32,383 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:50:36,179 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 03:50:36,179 | [33mPress CTRL+C to quit[0m
2025-11-17 03:50:36,180 |  * Restarting with stat
2025-11-17 03:50:54,667 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:50:54,801 | Use pytorch device_name: cpu
2025-11-17 03:50:54,801 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:50:58,948 |  * Debugger is active!
2025-11-17 03:50:58,952 |  * Debugger PIN: 136-249-302
2025-11-17 03:52:56,617 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 03:52:56,620 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 127, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 03:52:56,691 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=1.27s
2025-11-17 03:52:56,692 | 192.168.2.125 - - [17/Nov/2025 03:52:56] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 03:55:24,404 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 03:55:26,463 |  * Restarting with stat
2025-11-17 03:55:43,294 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:55:43,395 | Use pytorch device_name: cpu
2025-11-17 03:55:43,395 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:55:47,653 |  * Debugger is active!
2025-11-17 03:55:47,656 |  * Debugger PIN: 136-249-302
2025-11-17 03:57:50,079 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:57:50,195 | Use pytorch device_name: cpu
2025-11-17 03:57:50,195 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:57:54,163 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 03:57:54,163 | [33mPress CTRL+C to quit[0m
2025-11-17 03:57:54,165 |  * Restarting with stat
2025-11-17 03:58:11,884 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 03:58:12,060 | Use pytorch device_name: cpu
2025-11-17 03:58:12,060 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 03:58:16,355 |  * Debugger is active!
2025-11-17 03:58:16,367 |  * Debugger PIN: 136-249-302
2025-11-17 04:02:09,051 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:02:11,325 |  * Restarting with stat
2025-11-17 04:02:47,144 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:02:47,256 | Use pytorch device_name: cpu
2025-11-17 04:02:47,257 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:02:53,316 | HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-11-17 04:02:53,344 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 04:02:53,344 | [33mPress CTRL+C to quit[0m
2025-11-17 04:02:53,346 |  * Restarting with stat
2025-11-17 04:03:09,088 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:03:09,209 | Use pytorch device_name: cpu
2025-11-17 04:03:09,211 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:03:15,722 | HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-11-17 04:03:15,735 |  * Debugger is active!
2025-11-17 04:03:15,740 |  * Debugger PIN: 136-249-302
2025-11-17 04:03:16,394 | HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 04:03:16,396 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 136, in chat
    response = ollama_client.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 04:03:16,456 | user=HZjR9WyUAuCFsUjFIgLH | len=10 | time=0.70s
2025-11-17 04:03:16,457 | 192.168.2.125 - - [17/Nov/2025 04:03:16] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:03:27,183 | HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 04:03:27,184 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 136, in chat
    response = ollama_client.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 04:03:27,199 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=2.32s
2025-11-17 04:03:27,199 | 192.168.2.125 - - [17/Nov/2025 04:03:27] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:09:28,460 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:09:31,111 |  * Restarting with stat
2025-11-17 04:09:51,210 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:09:51,351 | Use pytorch device_name: cpu
2025-11-17 04:09:51,351 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:09:55,734 |  * Debugger is active!
2025-11-17 04:09:55,742 |  * Debugger PIN: 136-249-302
2025-11-17 04:10:29,018 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:10:30,574 |  * Restarting with stat
2025-11-17 04:11:03,603 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:11:03,716 | Use pytorch device_name: cpu
2025-11-17 04:11:03,716 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:11:08,131 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 04:11:08,131 | [33mPress CTRL+C to quit[0m
2025-11-17 04:11:08,133 |  * Restarting with stat
2025-11-17 04:11:23,382 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:11:23,503 | Use pytorch device_name: cpu
2025-11-17 04:11:23,503 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:11:27,639 |  * Debugger is active!
2025-11-17 04:11:27,643 |  * Debugger PIN: 136-249-302
2025-11-17 04:13:18,718 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:13:20,533 |  * Restarting with stat
2025-11-17 04:13:35,094 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:13:35,234 | Use pytorch device_name: cpu
2025-11-17 04:13:35,234 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:13:41,405 | HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-11-17 04:13:41,416 |  * Debugger is active!
2025-11-17 04:13:41,420 |  * Debugger PIN: 136-249-302
2025-11-17 04:14:14,204 | HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 04:14:14,207 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 136, in chat
    response = ollama_client.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 04:14:14,259 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=2.55s
2025-11-17 04:14:14,261 | 192.168.2.125 - - [17/Nov/2025 04:14:14] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:19:58,541 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:20:02,675 |  * Restarting with stat
2025-11-17 04:20:29,186 | INFO | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:20:29,561 | INFO | Use pytorch device_name: cpu
2025-11-17 04:20:29,561 | INFO | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:20:33,965 | WARNING |  * Debugger is active!
2025-11-17 04:20:33,969 | INFO |  * Debugger PIN: 136-249-302
2025-11-17 04:21:02,193 | ERROR | LLM generation error: Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x000001E2DD769BD0>: Failed to resolve 'api.ollama.ai' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 486, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.ollama.ai', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001E2DD769BD0>: Failed to resolve 'api.ollama.ai' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 93, in call_ollama_cloud
    r = requests.post(OLLAMA_CLOUD_URL, headers=headers, json=payload, timeout=HTTP_TIMEOUT)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.ollama.ai', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001E2DD769BD0>: Failed to resolve 'api.ollama.ai' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 235, in chat
    reply_text = generate_reply_from_llm(messages, prefer="auto", temperature=0.3)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 166, in generate_reply_from_llm
    return call_ollama_cloud(messages, temperature=temperature)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 95, in call_ollama_cloud
    raise RuntimeError(f"Network error calling Ollama Cloud: {e}")
RuntimeError: Network error calling Ollama Cloud: HTTPSConnectionPool(host='api.ollama.ai', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001E2DD769BD0>: Failed to resolve 'api.ollama.ai' ([Errno 11001] getaddrinfo failed)"))

2025-11-17 04:21:02,261 | INFO | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=0.50s
2025-11-17 04:21:02,262 | INFO | 192.168.2.125 - - [17/Nov/2025 04:21:02] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:22:32,178 | INFO |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:22:34,487 |  * Restarting with stat
2025-11-17 04:22:53,425 | INFO | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:22:53,542 | INFO | Use pytorch device_name: cpu
2025-11-17 04:22:53,542 | INFO | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:22:57,353 | WARNING |  * Debugger is active!
2025-11-17 04:22:57,358 | INFO |  * Debugger PIN: 136-249-302
2025-11-17 04:23:02,431 | INFO |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 04:23:04,711 |  * Restarting with stat
2025-11-17 04:23:44,046 | INFO | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:23:44,155 | INFO | Use pytorch device_name: cpu
2025-11-17 04:23:44,155 | INFO | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:23:47,988 | INFO | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 04:23:47,988 | INFO | [33mPress CTRL+C to quit[0m
2025-11-17 04:23:47,989 | INFO |  * Restarting with stat
2025-11-17 04:24:06,238 | INFO | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 04:24:06,354 | INFO | Use pytorch device_name: cpu
2025-11-17 04:24:06,354 | INFO | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 04:24:10,242 | WARNING |  * Debugger is active!
2025-11-17 04:24:10,256 | INFO |  * Debugger PIN: 136-249-302
2025-11-17 04:24:10,420 | ERROR | LLM generation error: Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x00000178E3321D10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 486, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3321D10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 93, in call_ollama_cloud
    r = requests.post(OLLAMA_CLOUD_URL, headers=headers, json=payload, timeout=HTTP_TIMEOUT)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3321D10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 235, in chat
    reply_text = generate_reply_from_llm(messages, prefer="auto", temperature=0.3)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 166, in generate_reply_from_llm
    return call_ollama_cloud(messages, temperature=temperature)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 95, in call_ollama_cloud
    raise RuntimeError(f"Network error calling Ollama Cloud: {e}")
RuntimeError: Network error calling Ollama Cloud: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3321D10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

2025-11-17 04:24:10,462 | INFO | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=0.19s
2025-11-17 04:24:10,463 | INFO | 192.168.2.125 - - [17/Nov/2025 04:24:10] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:24:17,162 | ERROR | LLM generation error: Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x00000178E3322D50>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 486, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322D50>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 93, in call_ollama_cloud
    r = requests.post(OLLAMA_CLOUD_URL, headers=headers, json=payload, timeout=HTTP_TIMEOUT)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322D50>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 235, in chat
    reply_text = generate_reply_from_llm(messages, prefer="auto", temperature=0.3)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 166, in generate_reply_from_llm
    return call_ollama_cloud(messages, temperature=temperature)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 95, in call_ollama_cloud
    raise RuntimeError(f"Network error calling Ollama Cloud: {e}")
RuntimeError: Network error calling Ollama Cloud: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322D50>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

2025-11-17 04:24:17,187 | INFO | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=0.07s
2025-11-17 04:24:17,188 | INFO | 192.168.2.125 - - [17/Nov/2025 04:24:17] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 04:24:22,229 | ERROR | LLM generation error: Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x00000178E3322C10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 486, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322C10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 93, in call_ollama_cloud
    r = requests.post(OLLAMA_CLOUD_URL, headers=headers, json=payload, timeout=HTTP_TIMEOUT)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322C10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 235, in chat
    reply_text = generate_reply_from_llm(messages, prefer="auto", temperature=0.3)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 166, in generate_reply_from_llm
    return call_ollama_cloud(messages, temperature=temperature)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 95, in call_ollama_cloud
    raise RuntimeError(f"Network error calling Ollama Cloud: {e}")
RuntimeError: Network error calling Ollama Cloud: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000178E3322C10>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

2025-11-17 04:24:22,250 | INFO | user=HZjR9WyUAuCFsUjFIgLH | len=3 | time=0.05s
2025-11-17 04:24:22,250 | INFO | 192.168.2.125 - - [17/Nov/2025 04:24:22] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:12:47,147 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 08:12:47,437 | Use pytorch device_name: cpu
2025-11-17 08:12:47,437 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 08:12:55,053 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://10.242.233.83:3125
2025-11-17 08:12:55,053 | [33mPress CTRL+C to quit[0m
2025-11-17 08:12:55,056 |  * Restarting with stat
2025-11-17 08:13:12,759 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 08:13:12,880 | Use pytorch device_name: cpu
2025-11-17 08:13:12,881 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 08:13:18,424 |  * Debugger is active!
2025-11-17 08:13:18,429 |  * Debugger PIN: 136-249-302
2025-11-17 08:15:25,338 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:25,341 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:25,407 | user=FRjnep1OzM8mpMDiIiyu | len=193 | time=1.26s
2025-11-17 08:15:25,408 | 10.242.233.111 - - [17/Nov/2025 08:15:25] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:15:28,200 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:28,201 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:28,220 | user=Lx0v6alhIHxtyT0omFDh | len=193 | time=0.61s
2025-11-17 08:15:28,220 | 10.242.233.111 - - [17/Nov/2025 08:15:28] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:15:29,740 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:29,741 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:29,761 | user=UoPAelOGnpLkK3uk4Kb1 | len=193 | time=0.37s
2025-11-17 08:15:29,762 | 10.242.233.111 - - [17/Nov/2025 08:15:29] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:15:31,400 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:31,407 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:31,447 | user=VFVKIRlqixVhTsGE7lCh | len=193 | time=0.40s
2025-11-17 08:15:31,447 | 10.242.233.111 - - [17/Nov/2025 08:15:31] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:15:32,967 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:32,969 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:32,995 | user=k4IrYKtjRrB73qPy8CPG | len=193 | time=0.40s
2025-11-17 08:15:32,996 | 10.242.233.111 - - [17/Nov/2025 08:15:32] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:15:52,799 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:15:52,800 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:15:52,843 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=0.52s
2025-11-17 08:15:52,847 | 10.242.233.111 - - [17/Nov/2025 08:15:52] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:17:25,484 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:17:25,485 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:17:25,501 | user=HZjR9WyUAuCFsUjFIgLH | len=2 | time=0.58s
2025-11-17 08:17:25,502 | 10.242.233.111 - - [17/Nov/2025 08:17:25] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:17:57,800 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:17:57,801 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:17:57,814 | user=HZjR9WyUAuCFsUjFIgLH | len=2 | time=0.65s
2025-11-17 08:17:57,815 | 10.242.233.111 - - [17/Nov/2025 08:17:57] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:19:50,530 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:19:50,531 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 125, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:19:50,546 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=1.28s
2025-11-17 08:19:50,547 | 10.242.233.111 - - [17/Nov/2025 08:19:50] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:22:30,990 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 08:22:33,045 |  * Restarting with stat
2025-11-17 08:22:53,737 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 08:22:53,881 | Use pytorch device_name: cpu
2025-11-17 08:22:53,881 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 08:23:01,255 |  * Debugger is active!
2025-11-17 08:23:01,259 |  * Debugger PIN: 136-249-302
2025-11-17 08:23:13,393 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 08:23:15,151 |  * Restarting with stat
2025-11-17 08:23:30,583 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 08:23:30,750 | Use pytorch device_name: cpu
2025-11-17 08:23:30,750 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 08:23:36,230 |  * Debugger is active!
2025-11-17 08:23:36,238 |  * Debugger PIN: 136-249-302
2025-11-17 08:24:07,762 | HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 401 Unauthorized"
2025-11-17 08:24:07,775 | Ollama call error: Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 126, in chat
    response = ollama.chat(model=MODEL_NAME, messages=messages)
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 351, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 189, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\ollama\_client.py", line 133, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: unauthorized (status code: 401)

2025-11-17 08:24:07,820 | user=HZjR9WyUAuCFsUjFIgLH | len=5 | time=1.42s
2025-11-17 08:24:07,821 | 10.242.233.111 - - [17/Nov/2025 08:24:07] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:41:42,579 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 08:41:44,321 |  * Restarting with stat
2025-11-17 09:07:12,652 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 09:07:12,823 | Use pytorch device_name: cpu
2025-11-17 09:07:12,823 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 09:07:20,524 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://10.242.233.83:3125
2025-11-17 09:07:20,524 | [33mPress CTRL+C to quit[0m
2025-11-17 09:07:20,526 |  * Restarting with stat
2025-11-17 09:07:34,247 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 09:07:34,341 | Use pytorch device_name: cpu
2025-11-17 09:07:34,341 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 09:07:40,639 |  * Debugger is active!
2025-11-17 09:07:40,643 |  * Debugger PIN: 136-249-302
2025-11-17 09:08:55,706 | Ollama error: Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 488, in _make_request
    raise new_e
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x000001FC3FFD39D0>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 486, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "D:\Python Jupiter\Lib\site-packages\urllib3\util\retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001FC3FFD39D0>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 172, in chat
    reply = call_ollama_cloud(MODEL_NAME, messages)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 93, in call_ollama_cloud
    response = requests.post(OLLAMA_URL, headers=headers, json=payload)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001FC3FFD39D0>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))

2025-11-17 09:08:55,765 | auto-learn error
2025-11-17 09:08:55,766 | 10.242.233.111 - - [17/Nov/2025 09:08:55] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 07:17:41,345 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:17:41,563 | Use pytorch device_name: cpu
2025-11-17 07:17:41,563 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:17:46,730 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://10.60.82.45:3125
2025-11-17 07:17:46,730 | [33mPress CTRL+C to quit[0m
2025-11-17 07:17:46,732 |  * Restarting with stat
2025-11-17 07:18:09,355 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:18:09,501 | Use pytorch device_name: cpu
2025-11-17 07:18:09,501 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:18:14,259 |  * Debugger is active!
2025-11-17 07:18:14,274 |  * Debugger PIN: 136-249-302
2025-11-17 07:18:29,878 | Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 611, in get_json
    rv = self.json_module.loads(data)
  File "D:\Python Jupiter\Lib\site-packages\flask\json\provider.py", line 188, in loads
    return json.loads(s, **kwargs)
           ~~~~~~~~~~^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\json\__init__.py", line 341, in loads
    s = s.decode(detect_encoding(s), 'surrogatepass')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0 in position 37: invalid continuation byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 151, in chat
    data = request.json or {}
           ^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 560, in json
    return self.get_json()
           ~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 620, in get_json
    rv = self.on_json_loading_failed(e)
  File "D:\Python Jupiter\Lib\site-packages\flask\wrappers.py", line 130, in on_json_loading_failed
    return super().on_json_loading_failed(e)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 645, in on_json_loading_failed
    raise BadRequest(f"Failed to decode JSON object: {e}")
werkzeug.exceptions.BadRequest: 400 Bad Request: Failed to decode JSON object: 'utf-8' codec can't decode byte 0xe0 in position 37: invalid continuation byte

2025-11-17 07:18:29,879 | 127.0.0.1 - - [17/Nov/2025 07:18:29] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-11-17 07:19:42,355 | Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 152, in chat
    user_message = (data.get("message") or "").trim()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'trim'. Did you mean: 'strip'?

2025-11-17 07:19:42,356 | 127.0.0.1 - - [17/Nov/2025 07:19:42] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-11-17 07:20:03,611 | Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 152, in chat
    user_message = (data.get("message") or "").trim()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'trim'. Did you mean: 'strip'?

2025-11-17 07:20:03,611 | 127.0.0.1 - - [17/Nov/2025 07:20:03] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-11-17 07:22:02,812 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 07:22:05,417 |  * Restarting with stat
2025-11-17 07:22:29,674 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:22:29,856 | Use pytorch device_name: cpu
2025-11-17 07:22:29,856 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:22:38,816 |  * Debugger is active!
2025-11-17 07:22:38,820 |  * Debugger PIN: 136-249-302
2025-11-17 07:22:39,037 | AFC is enabled with max remote calls: 10.
2025-11-17 07:23:11,568 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 07:23:11,794 | auto-learn error
2025-11-17 07:23:11,796 | 127.0.0.1 - - [17/Nov/2025 07:23:11] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 07:38:04,749 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:38:04,858 | Use pytorch device_name: cpu
2025-11-17 07:38:04,858 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:38:16,641 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://10.60.82.45:3125
2025-11-17 07:38:16,642 | [33mPress CTRL+C to quit[0m
2025-11-17 07:38:16,643 |  * Restarting with stat
2025-11-17 07:38:31,884 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:38:32,003 | Use pytorch device_name: cpu
2025-11-17 07:38:32,003 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:38:40,112 |  * Debugger is active!
2025-11-17 07:38:40,115 |  * Debugger PIN: 136-249-302
2025-11-17 07:48:45,168 | AFC is enabled with max remote calls: 10.
2025-11-17 07:48:49,696 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 07:48:49,768 | auto-learn error
2025-11-17 07:48:49,770 | 127.0.0.1 - - [17/Nov/2025 07:48:49] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 07:53:40,315 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 07:53:40,489 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 07:53:46,370 |  * Restarting with stat
2025-11-17 07:53:46,779 |  * Restarting with stat
2025-11-17 07:54:09,308 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:54:09,418 | Use pytorch device_name: cpu
2025-11-17 07:54:09,418 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:54:09,779 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:54:09,967 | Use pytorch device_name: cpu
2025-11-17 07:54:09,967 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:54:14,119 |  * Debugger is active!
2025-11-17 07:54:14,122 |  * Debugger PIN: 136-249-302
2025-11-17 07:54:14,454 |  * Debugger is active!
2025-11-17 07:54:14,459 |  * Debugger PIN: 136-249-302
2025-11-17 07:56:50,398 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 07:56:51,007 |  * Detected change in 'D:\\Android\\Projects\\Nihongo\\backend AI\\nihongo.py', reloading
2025-11-17 07:56:56,759 |  * Restarting with stat
2025-11-17 07:56:57,072 |  * Restarting with stat
2025-11-17 07:57:17,810 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:57:17,940 | Use pytorch device_name: cpu
2025-11-17 07:57:17,940 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:57:22,735 |  * Debugger is active!
2025-11-17 07:57:22,738 |  * Debugger PIN: 136-249-302
2025-11-17 07:57:31,965 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:57:33,967 | Use pytorch device_name: cpu
2025-11-17 07:57:33,967 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:57:39,699 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://10.60.82.45:3125
2025-11-17 07:57:39,699 | [33mPress CTRL+C to quit[0m
2025-11-17 07:57:39,701 |  * Restarting with stat
2025-11-17 07:57:55,885 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 07:57:55,985 | Use pytorch device_name: cpu
2025-11-17 07:57:55,986 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 07:58:01,380 |  * Debugger is active!
2025-11-17 07:58:01,403 |  * Debugger PIN: 136-249-302
2025-11-17 08:05:22,405 | Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 611, in get_json
    rv = self.json_module.loads(data)
  File "D:\Python Jupiter\Lib\site-packages\flask\json\provider.py", line 188, in loads
    return json.loads(s, **kwargs)
           ~~~~~~~~~~^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\json\__init__.py", line 341, in loads
    s = s.decode(detect_encoding(s), 'surrogatepass')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0 in position 55: invalid continuation byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 151, in chat
    data = request.json or {}
           ^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 560, in json
    return self.get_json()
           ~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 620, in get_json
    rv = self.on_json_loading_failed(e)
  File "D:\Python Jupiter\Lib\site-packages\flask\wrappers.py", line 130, in on_json_loading_failed
    return super().on_json_loading_failed(e)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 645, in on_json_loading_failed
    raise BadRequest(f"Failed to decode JSON object: {e}")
werkzeug.exceptions.BadRequest: 400 Bad Request: Failed to decode JSON object: 'utf-8' codec can't decode byte 0xe0 in position 55: invalid continuation byte

2025-11-17 08:05:22,406 | 10.60.82.45 - - [17/Nov/2025 08:05:22] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-11-17 08:07:09,653 | Traceback (most recent call last):
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 611, in get_json
    rv = self.json_module.loads(data)
  File "D:\Python Jupiter\Lib\site-packages\flask\json\provider.py", line 188, in loads
    return json.loads(s, **kwargs)
           ~~~~~~~~~~^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\json\__init__.py", line 341, in loads
    s = s.decode(detect_encoding(s), 'surrogatepass')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0 in position 55: invalid continuation byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 151, in chat
    data = request.json or {}
           ^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 560, in json
    return self.get_json()
           ~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 620, in get_json
    rv = self.on_json_loading_failed(e)
  File "D:\Python Jupiter\Lib\site-packages\flask\wrappers.py", line 130, in on_json_loading_failed
    return super().on_json_loading_failed(e)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "D:\Python Jupiter\Lib\site-packages\werkzeug\wrappers\request.py", line 645, in on_json_loading_failed
    raise BadRequest(f"Failed to decode JSON object: {e}")
werkzeug.exceptions.BadRequest: 400 Bad Request: Failed to decode JSON object: 'utf-8' codec can't decode byte 0xe0 in position 55: invalid continuation byte

2025-11-17 08:07:09,654 | 10.60.82.45 - - [17/Nov/2025 08:07:09] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-11-17 08:07:35,874 | AFC is enabled with max remote calls: 10.
2025-11-17 08:07:37,921 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 08:07:38,016 | auto-learn error
2025-11-17 08:07:38,016 | 10.60.82.45 - - [17/Nov/2025 08:07:38] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 08:07:58,198 | 10.60.82.45 - - [17/Nov/2025 08:07:58] "[31m[1mGET /api/chat HTTP/1.1[0m" 405 -
2025-11-17 08:08:00,164 | 10.60.82.45 - - [17/Nov/2025 08:08:00] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-11-17 22:04:08,020 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 22:04:08,392 | Use pytorch device_name: cpu
2025-11-17 22:04:08,392 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 22:04:23,725 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 22:04:23,725 | [33mPress CTRL+C to quit[0m
2025-11-17 22:04:23,728 |  * Restarting with stat
2025-11-17 22:04:48,201 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 22:04:48,333 | Use pytorch device_name: cpu
2025-11-17 22:04:48,333 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 22:04:52,524 |  * Debugger is active!
2025-11-17 22:04:52,528 |  * Debugger PIN: 136-249-302
2025-11-17 22:08:49,213 | 127.0.0.1 - - [17/Nov/2025 22:08:49] "[33mGET / HTTP/1.1[0m" 404 -
2025-11-17 22:08:49,923 | 127.0.0.1 - - [17/Nov/2025 22:08:49] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-11-17 22:11:20,650 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:22,252 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:22,340 | auto-learn error
2025-11-17 22:11:22,341 | 127.0.0.1 - - [17/Nov/2025 22:11:22] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:37,816 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:40,060 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:40,261 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:40,338 | auto-learn error
2025-11-17 22:11:40,339 | 127.0.0.1 - - [17/Nov/2025 22:11:40] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:41,190 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:41,267 | auto-learn error
2025-11-17 22:11:41,268 | 127.0.0.1 - - [17/Nov/2025 22:11:41] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:41,415 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:43,219 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:43,280 | auto-learn error
2025-11-17 22:11:43,281 | 127.0.0.1 - - [17/Nov/2025 22:11:43] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:44,205 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:46,072 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:46,121 | auto-learn error
2025-11-17 22:11:46,122 | 127.0.0.1 - - [17/Nov/2025 22:11:46] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:47,053 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:49,034 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:49,096 | auto-learn error
2025-11-17 22:11:49,097 | 127.0.0.1 - - [17/Nov/2025 22:11:49] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 22:11:50,030 | AFC is enabled with max remote calls: 10.
2025-11-17 22:11:51,780 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 22:11:51,897 | auto-learn error
2025-11-17 22:11:51,899 | 127.0.0.1 - - [17/Nov/2025 22:11:51] "POST /api/chat HTTP/1.1" 200 -
2025-11-17 23:13:49,107 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 23:13:49,420 | Use pytorch device_name: cpu
2025-11-17 23:13:49,420 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 23:13:53,260 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 23:13:53,260 | [33mPress CTRL+C to quit[0m
2025-11-17 23:13:53,262 |  * Restarting with stat
2025-11-17 23:14:16,589 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 23:14:16,690 | Use pytorch device_name: cpu
2025-11-17 23:14:16,690 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 23:14:20,645 |  * Debugger is active!
2025-11-17 23:14:20,650 |  * Debugger PIN: 136-249-302
2025-11-17 23:38:38,343 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 23:38:38,719 | Use pytorch device_name: cpu
2025-11-17 23:38:38,719 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 23:38:44,085 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.2.62:3125
2025-11-17 23:38:44,085 | [33mPress CTRL+C to quit[0m
2025-11-17 23:38:44,087 |  * Restarting with stat
2025-11-17 23:39:06,225 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-17 23:39:06,358 | Use pytorch device_name: cpu
2025-11-17 23:39:06,358 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-17 23:39:10,188 |  * Debugger is active!
2025-11-17 23:39:10,198 |  * Debugger PIN: 136-249-302
2025-11-17 23:40:58,790 | AFC is enabled with max remote calls: 10.
2025-11-17 23:41:01,489 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-17 23:41:01,654 | auto-learn error
2025-11-17 23:41:01,655 | 127.0.0.1 - - [17/Nov/2025 23:41:01] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:00:54,992 | AFC is enabled with max remote calls: 10.
2025-11-18 00:00:56,973 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:00:57,069 | auto-learn error
2025-11-18 00:00:57,071 | 127.0.0.1 - - [18/Nov/2025 00:00:57] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:00:58,389 | AFC is enabled with max remote calls: 10.
2025-11-18 00:01:00,139 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:01:00,186 | auto-learn error
2025-11-18 00:01:00,186 | 127.0.0.1 - - [18/Nov/2025 00:01:00] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:01:01,553 | AFC is enabled with max remote calls: 10.
2025-11-18 00:01:03,491 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:01:03,542 | auto-learn error
2025-11-18 00:01:03,542 | 127.0.0.1 - - [18/Nov/2025 00:01:03] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:01:04,749 | AFC is enabled with max remote calls: 10.
2025-11-18 00:01:06,603 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:01:06,659 | auto-learn error
2025-11-18 00:01:06,659 | 127.0.0.1 - - [18/Nov/2025 00:01:06] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:02:02,317 | AFC is enabled with max remote calls: 10.
2025-11-18 00:02:04,541 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:02:04,647 | auto-learn error
2025-11-18 00:02:04,648 | 127.0.0.1 - - [18/Nov/2025 00:02:04] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:35:32,659 | 127.0.0.1 - - [18/Nov/2025 00:35:32] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:36:05,104 | 127.0.0.1 - - [18/Nov/2025 00:36:05] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:36:23,749 | 127.0.0.1 - - [18/Nov/2025 00:36:23] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:42:00,011 | AFC is enabled with max remote calls: 10.
2025-11-18 00:42:01,681 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:42:01,789 | auto-learn error
2025-11-18 00:42:01,790 | 127.0.0.1 - - [18/Nov/2025 00:42:01] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 00:53:58,494 | AFC is enabled with max remote calls: 10.
2025-11-18 00:54:00,101 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 00:54:00,244 | auto-learn error
2025-11-18 00:54:00,246 | 127.0.0.1 - - [18/Nov/2025 00:54:00] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 01:00:52,486 | AFC is enabled with max remote calls: 10.
2025-11-18 01:00:54,026 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 01:00:54,140 | auto-learn error
2025-11-18 01:00:54,143 | 127.0.0.1 - - [18/Nov/2025 01:00:54] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 01:06:27,790 | AFC is enabled with max remote calls: 10.
2025-11-18 01:06:29,128 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 01:06:29,224 | auto-learn error
2025-11-18 01:06:29,226 | 127.0.0.1 - - [18/Nov/2025 01:06:29] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:22:05,410 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 13:22:05,614 | Use pytorch device_name: cpu
2025-11-18 13:22:05,614 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 13:22:11,670 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.1.147:3125
2025-11-18 13:22:11,670 | [33mPress CTRL+C to quit[0m
2025-11-18 13:22:11,672 |  * Restarting with stat
2025-11-18 13:22:28,617 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 13:22:28,723 | Use pytorch device_name: cpu
2025-11-18 13:22:28,723 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 13:22:33,486 |  * Debugger is active!
2025-11-18 13:22:33,491 |  * Debugger PIN: 136-249-302
2025-11-18 13:27:14,732 | 127.0.0.1 - - [18/Nov/2025 13:27:14] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:16,136 | AFC is enabled with max remote calls: 10.
2025-11-18 13:27:18,863 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:27:19,021 | auto-learn error
2025-11-18 13:27:19,021 | 127.0.0.1 - - [18/Nov/2025 13:27:19] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:22,070 | AFC is enabled with max remote calls: 10.
2025-11-18 13:27:25,209 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 429 Too Many Requests"
2025-11-18 13:27:25,411 | Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 192, in chat
    reply = call_gemini(MODEL_NAME, messages)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 109, in call_gemini
    response = client.models.generate_content(
        model=model,
        contents=final_prompt
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\models.py", line 5001, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\models.py", line 3813, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1307, in request
    response = self._request(http_request, http_options, stream=False)
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1143, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1120, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\google\genai\errors.py", line 108, in raise_for_response
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}

2025-11-18 13:27:25,447 | auto-learn error
2025-11-18 13:27:25,447 | 127.0.0.1 - - [18/Nov/2025 13:27:25] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:30,323 | AFC is enabled with max remote calls: 10.
2025-11-18 13:27:35,953 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:27:36,040 | auto-learn error
2025-11-18 13:27:36,041 | 127.0.0.1 - - [18/Nov/2025 13:27:36] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:38,843 | AFC is enabled with max remote calls: 10.
2025-11-18 13:27:40,989 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 429 Too Many Requests"
2025-11-18 13:27:40,992 | Traceback (most recent call last):
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 192, in chat
    reply = call_gemini(MODEL_NAME, messages)
  File "D:\Android\Projects\Nihongo\backend AI\nihongo.py", line 109, in call_gemini
    response = client.models.generate_content(
        model=model,
        contents=final_prompt
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\models.py", line 5001, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\models.py", line 3813, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1307, in request
    response = self._request(http_request, http_options, stream=False)
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1143, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "D:\Python Jupiter\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\Python Jupiter\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "D:\Python Jupiter\Lib\site-packages\google\genai\_api_client.py", line 1120, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\Python Jupiter\Lib\site-packages\google\genai\errors.py", line 108, in raise_for_response
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}

2025-11-18 13:27:41,030 | auto-learn error
2025-11-18 13:27:41,030 | 127.0.0.1 - - [18/Nov/2025 13:27:41] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:43,021 | AFC is enabled with max remote calls: 10.
2025-11-18 13:27:45,798 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:27:45,871 | auto-learn error
2025-11-18 13:27:45,871 | 127.0.0.1 - - [18/Nov/2025 13:27:45] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:27:57,746 | 127.0.0.1 - - [18/Nov/2025 13:27:57] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:28:12,996 | 127.0.0.1 - - [18/Nov/2025 13:28:12] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:28:18,132 | AFC is enabled with max remote calls: 10.
2025-11-18 13:28:21,383 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:28:21,452 | auto-learn error
2025-11-18 13:28:21,453 | 127.0.0.1 - - [18/Nov/2025 13:28:21] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:31:43,670 | 127.0.0.1 - - [18/Nov/2025 13:31:43] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:34:32,883 | 127.0.0.1 - - [18/Nov/2025 13:34:32] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:34:55,068 | 127.0.0.1 - - [18/Nov/2025 13:34:55] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:35:03,078 | 127.0.0.1 - - [18/Nov/2025 13:35:03] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:05,392 | 127.0.0.1 - - [18/Nov/2025 13:39:05] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:06,972 | AFC is enabled with max remote calls: 10.
2025-11-18 13:39:08,422 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:39:08,460 | auto-learn error
2025-11-18 13:39:08,460 | 127.0.0.1 - - [18/Nov/2025 13:39:08] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:10,487 | AFC is enabled with max remote calls: 10.
2025-11-18 13:39:11,916 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:39:11,960 | auto-learn error
2025-11-18 13:39:11,961 | 127.0.0.1 - - [18/Nov/2025 13:39:11] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:14,080 | AFC is enabled with max remote calls: 10.
2025-11-18 13:39:15,171 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:39:15,201 | auto-learn error
2025-11-18 13:39:15,202 | 127.0.0.1 - - [18/Nov/2025 13:39:15] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:17,441 | AFC is enabled with max remote calls: 10.
2025-11-18 13:39:18,333 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:39:18,365 | auto-learn error
2025-11-18 13:39:18,366 | 127.0.0.1 - - [18/Nov/2025 13:39:18] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:39:20,302 | AFC is enabled with max remote calls: 10.
2025-11-18 13:39:22,994 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:39:23,031 | auto-learn error
2025-11-18 13:39:23,031 | 127.0.0.1 - - [18/Nov/2025 13:39:23] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:41:04,380 | AFC is enabled with max remote calls: 10.
2025-11-18 13:41:06,060 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:41:06,107 | auto-learn error
2025-11-18 13:41:06,107 | 127.0.0.1 - - [18/Nov/2025 13:41:06] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:41:11,606 | AFC is enabled with max remote calls: 10.
2025-11-18 13:41:13,911 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:41:13,956 | auto-learn error
2025-11-18 13:41:13,956 | 127.0.0.1 - - [18/Nov/2025 13:41:13] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:41:24,094 | AFC is enabled with max remote calls: 10.
2025-11-18 13:41:30,596 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:41:30,639 | auto-learn error
2025-11-18 13:41:30,640 | 127.0.0.1 - - [18/Nov/2025 13:41:30] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 13:41:36,207 | AFC is enabled with max remote calls: 10.
2025-11-18 13:41:38,297 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 13:41:38,343 | auto-learn error
2025-11-18 13:41:38,343 | 127.0.0.1 - - [18/Nov/2025 13:41:38] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:21:52,520 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 14:21:52,798 | Use pytorch device_name: cpu
2025-11-18 14:21:52,798 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 14:21:57,981 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.1.147:3125
2025-11-18 14:21:57,981 | [33mPress CTRL+C to quit[0m
2025-11-18 14:21:57,983 |  * Restarting with stat
2025-11-18 14:22:20,010 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 14:22:20,159 | Use pytorch device_name: cpu
2025-11-18 14:22:20,159 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 14:22:27,262 |  * Debugger is active!
2025-11-18 14:22:27,269 |  * Debugger PIN: 136-249-302
2025-11-18 14:22:27,451 | AFC is enabled with max remote calls: 10.
2025-11-18 14:22:27,457 | AFC is enabled with max remote calls: 10.
2025-11-18 14:22:29,266 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:22:29,387 | auto-learn error
2025-11-18 14:22:29,389 | 127.0.0.1 - - [18/Nov/2025 14:22:29] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:22:35,621 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:22:35,687 | auto-learn error
2025-11-18 14:22:35,687 | 127.0.0.1 - - [18/Nov/2025 14:22:35] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:22:47,439 | AFC is enabled with max remote calls: 10.
2025-11-18 14:22:49,804 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:22:49,861 | auto-learn error
2025-11-18 14:22:49,861 | 127.0.0.1 - - [18/Nov/2025 14:22:49] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:23:00,876 | AFC is enabled with max remote calls: 10.
2025-11-18 14:23:04,655 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:23:04,767 | auto-learn error
2025-11-18 14:23:04,767 | 127.0.0.1 - - [18/Nov/2025 14:23:04] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:23:47,684 | AFC is enabled with max remote calls: 10.
2025-11-18 14:23:59,754 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:23:59,815 | auto-learn error
2025-11-18 14:23:59,815 | 127.0.0.1 - - [18/Nov/2025 14:23:59] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:24:16,436 | AFC is enabled with max remote calls: 10.
2025-11-18 14:24:30,790 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:24:30,846 | auto-learn error
2025-11-18 14:24:30,846 | 127.0.0.1 - - [18/Nov/2025 14:24:30] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:30:19,316 | AFC is enabled with max remote calls: 10.
2025-11-18 14:30:30,148 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:30:30,227 | auto-learn error
2025-11-18 14:30:30,227 | 127.0.0.1 - - [18/Nov/2025 14:30:30] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:31:03,717 | AFC is enabled with max remote calls: 10.
2025-11-18 14:31:13,728 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:31:13,797 | auto-learn error
2025-11-18 14:31:13,798 | 127.0.0.1 - - [18/Nov/2025 14:31:13] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:39:15,803 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 14:39:16,013 | Use pytorch device_name: cpu
2025-11-18 14:39:16,013 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 14:39:24,489 | [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:3125
 * Running on http://192.168.1.147:3125
2025-11-18 14:39:24,489 | [33mPress CTRL+C to quit[0m
2025-11-18 14:39:24,491 |  * Restarting with stat
2025-11-18 14:39:41,738 | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-11-18 14:39:41,836 | Use pytorch device_name: cpu
2025-11-18 14:39:41,836 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-11-18 14:39:49,914 |  * Debugger is active!
2025-11-18 14:39:49,918 |  * Debugger PIN: 136-249-302
2025-11-18 14:39:56,963 | AFC is enabled with max remote calls: 10.
2025-11-18 14:40:01,628 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:40:01,715 | auto-learn error
2025-11-18 14:40:01,717 | 127.0.0.1 - - [18/Nov/2025 14:40:01] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:40:25,871 | AFC is enabled with max remote calls: 10.
2025-11-18 14:40:29,844 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:40:29,892 | auto-learn error
2025-11-18 14:40:29,892 | 127.0.0.1 - - [18/Nov/2025 14:40:29] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:40:41,822 | AFC is enabled with max remote calls: 10.
2025-11-18 14:40:45,192 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:40:45,309 | auto-learn error
2025-11-18 14:40:45,309 | 127.0.0.1 - - [18/Nov/2025 14:40:45] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:40:54,252 | AFC is enabled with max remote calls: 10.
2025-11-18 14:40:59,435 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:40:59,467 | auto-learn error
2025-11-18 14:40:59,468 | 127.0.0.1 - - [18/Nov/2025 14:40:59] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:41:16,780 | AFC is enabled with max remote calls: 10.
2025-11-18 14:41:20,046 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:41:20,081 | auto-learn error
2025-11-18 14:41:20,081 | 127.0.0.1 - - [18/Nov/2025 14:41:20] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:10,422 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:15,320 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:15,371 | auto-learn error
2025-11-18 14:44:15,371 | 127.0.0.1 - - [18/Nov/2025 14:44:15] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:44,138 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:45,323 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:45,366 | auto-learn error
2025-11-18 14:44:45,367 | 127.0.0.1 - - [18/Nov/2025 14:44:45] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:47,191 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:48,147 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:48,199 | auto-learn error
2025-11-18 14:44:48,200 | 127.0.0.1 - - [18/Nov/2025 14:44:48] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:50,333 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:51,294 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:51,529 | auto-learn error
2025-11-18 14:44:51,530 | 127.0.0.1 - - [18/Nov/2025 14:44:51] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:54,597 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:55,595 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:55,711 | auto-learn error
2025-11-18 14:44:55,712 | 127.0.0.1 - - [18/Nov/2025 14:44:55] "POST /api/chat HTTP/1.1" 200 -
2025-11-18 14:44:58,193 | AFC is enabled with max remote calls: 10.
2025-11-18 14:44:59,177 | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-18 14:44:59,329 | auto-learn error
2025-11-18 14:44:59,329 | 127.0.0.1 - - [18/Nov/2025 14:44:59] "POST /api/chat HTTP/1.1" 200 -
